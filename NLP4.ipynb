{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPBm0QLxoa2f5WGntQAAch",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andysingal/Natural_Language_NLP/blob/main/NLP4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The topics that we will cover in this chapter include the following:\n",
        "<ul>\n",
        "   <li>Cleaning and preparing text data</li>\n",
        "   <li>Building feature vectors from text documents</li>\n",
        "   <li>Training a machine learning model to classify positive and negative movie reviews</li>\n",
        "   <li>Working with large text datasets using out-of-core learning</li>\n",
        "   <li>Inferring topics from document collections for categorization</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "qNOWTnKwDlzb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing the IMDb movie review data for text processing\n",
        "<p>As mentioned, sentiment analysis, sometimes also called opinion mining, is a popular subdiscipline of the broader field of NLP; it is concerned with analyzing the sentiment of documents.</p>"
      ],
      "metadata": {
        "id": "_O14uirBEnPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A compressed archive of the movie review dataset (84.1 MB) can be downloaded from http://ai.stanford.edu/~amaas/data/sentiment/ as a gzip-compressed tarball archive"
      ],
      "metadata": {
        "id": "VgHTa9XME4l9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pyprind"
      ],
      "metadata": {
        "id": "73U5RnD_F6Xx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sJ3t1i6Dcnn",
        "outputId": "62433567-0416-480b-a7db-e9ee17aed51c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% | 80.23 MB | 3.07 MB/s | 26.17 sec elapsed"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import tarfile\n",
        "import time\n",
        "import urllib.request\n",
        "\n",
        "source = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "target = 'aclImdb_v1.tar.gz'\n",
        "\n",
        "if os.path.exists(target):\n",
        "    os.remove(target)\n",
        "\n",
        "def reporthook(count, block_size, total_size):\n",
        "    global start_time\n",
        "    if count == 0:\n",
        "        start_time = time.time()\n",
        "        return\n",
        "    duration = time.time() - start_time\n",
        "    progress_size = int(count * block_size)\n",
        "    speed = progress_size / (1024.**2 * duration)\n",
        "    percent = count * block_size * 100. / total_size\n",
        "\n",
        "    sys.stdout.write(f'\\r{int(percent)}% | {progress_size / (1024.**2):.2f} MB '\n",
        "                     f'| {speed:.2f} MB/s | {duration:.2f} sec elapsed')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "if not os.path.isdir('aclImdb') and not os.path.isfile('aclImdb_v1.tar.gz'):\n",
        "    urllib.request.urlretrieve(source, target, reporthook)\n",
        "\n",
        "if not os.path.isdir('aclImdb'):\n",
        "\n",
        "    with tarfile.open(target, 'r:gz') as tar:\n",
        "        tar.extractall()    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyprind\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "from packaging import version\n",
        "\n",
        "\n",
        "# change the `basepath` to the directory of the\n",
        "# unzipped movie dataset\n",
        "\n",
        "basepath = 'aclImdb'\n",
        "\n",
        "labels = {'pos': 1, 'neg': 0}\n",
        "\n",
        "# if the progress bar does not show, change stream=sys.stdout to stream=2\n",
        "pbar = pyprind.ProgBar(50000, stream=sys.stdout)\n",
        "\n",
        "df = pd.DataFrame()\n",
        "for s in ('test', 'train'):\n",
        "    for l in ('pos', 'neg'):\n",
        "        path = os.path.join(basepath, s, l)\n",
        "        for file in sorted(os.listdir(path)):\n",
        "            with open(os.path.join(path, file), \n",
        "                      'r', encoding='utf-8') as infile:\n",
        "                txt = infile.read()\n",
        "                \n",
        "            if version.parse(pd.__version__) >= version.parse(\"1.3.2\"):\n",
        "                x = pd.DataFrame([[txt, labels[l]]], columns=['review', 'sentiment'])\n",
        "                df = pd.concat([df, x], ignore_index=False)\n",
        "\n",
        "            else:\n",
        "                df = df.append([[txt, labels[l]]], \n",
        "                               ignore_index=True)\n",
        "            pbar.update()\n",
        "df.columns = ['review', 'sentiment']"
      ],
      "metadata": {
        "id": "CHMCqyryFRZX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SAMPLE OF DATA"
      ],
      "metadata": {
        "id": "WbC907iZGuBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "if version.parse(pd.__version__) >= version.parse(\"1.3.2\"):\n",
        "    df = df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
        "    \n",
        "else:\n",
        "    np.random.seed(0)\n",
        "    df = df.reindex(np.random.permutation(df.index))"
      ],
      "metadata": {
        "id": "196ezlACGsc2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('movie_data.csv', index=False, encoding='utf-8')"
      ],
      "metadata": {
        "id": "LE-MLm0UFvVy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
        "\n",
        "# the following is necessary on some computers:\n",
        "df = df.rename(columns={\"0\": \"review\", \"1\": \"sentiment\"})\n",
        "\n",
        "df.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "OU1qEWOuHfhe",
        "outputId": "08bdf79a-1e4c-49f7-ad91-760d281cd18f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  sentiment\n",
              "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
              "1  OK... so... I really like Kris Kristofferson a...          0\n",
              "2  ***SPOILER*** Do not read this, if you think a...          0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-29f9844f-3ca2-4db4-b626-de94247bd1e9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-29f9844f-3ca2-4db4-b626-de94247bd1e9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-29f9844f-3ca2-4db4-b626-de94247bd1e9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-29f9844f-3ca2-4db4-b626-de94247bd1e9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introducing the bag-of-words model\n",
        "...\n",
        "\n",
        "<h2>Transforming documents into feature vectors</h2>\n",
        "By calling the fit_transform method on CountVectorizer, we just constructed the vocabulary of the bag-of-words model and transformed the following three sentences into sparse feature vectors:"
      ],
      "metadata": {
        "id": "td16g9cuIL8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sequences of items in NLP—words, letters, or symbols—are also called n-grams\n",
        "\n",
        "<p>The CountVectorizer class in scikit-learn allows us to use different n-gram models via its ngram_range parameter. While a 1-gram representation is used by default, we could switch to a 2-gram representation by initializing a new CountVectorizer instance with ngram_range=(2,2).</p>"
      ],
      "metadata": {
        "id": "2TpthmDDJb3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count = CountVectorizer()\n",
        "docs = np.array([\n",
        "        'The sun is shining',\n",
        "        'The weather is sweet',\n",
        "        'The sun is shining, the weather is sweet, and one and one is two'])\n",
        "bag = count.fit_transform(docs)"
      ],
      "metadata": {
        "id": "ELO3vewVHuSI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(count.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ka9rw3WXJ2t8",
        "outputId": "55428cda-27ea-45df-c7f0-50a598b30a42"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Assessing word relevancy via term frequency-inverse document frequency"
      ],
      "metadata": {
        "id": "5tKl1Nm5KFXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf = TfidfTransformer(use_idf=True, \n",
        "                         norm='l2', \n",
        "                         smooth_idf=True)\n",
        "print(tfidf.fit_transform(count.fit_transform(docs))\n",
        "      .toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DziWstIFJ-oh",
        "outputId": "c8b4e184-b079-41d9-d6d9-4bfbf63c1fc9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.43370786 0.         0.55847784 0.55847784 0.\n",
            "  0.43370786 0.         0.        ]\n",
            " [0.         0.43370786 0.         0.         0.         0.55847784\n",
            "  0.43370786 0.         0.55847784]\n",
            " [0.50238645 0.44507629 0.50238645 0.19103892 0.19103892 0.19103892\n",
            "  0.29671753 0.25119322 0.19103892]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cleaning text data\n"
      ],
      "metadata": {
        "id": "trxOTnR0KU8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def preprocessor(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
        "                           text)\n",
        "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
        "            ' '.join(emoticons).replace('-', ''))\n",
        "    return text"
      ],
      "metadata": {
        "id": "faR1Qm_SKK7g"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor(df.loc[0, 'review'][-50:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "kPi8c5cUKe0j",
        "outputId": "3fbfaa89-e9d7-498f-cc76-491168f83573"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'is seven title brazil not available'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor(\"</a>This :) is :( a test :-)!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "eMndDpb5KhmP",
        "outputId": "5982a9f2-b90a-4a55-fd32-70f4cfc16092"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'this is a test :) :( :)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['review'] = df['review'].apply(preprocessor)"
      ],
      "metadata": {
        "id": "tskxk5eiKtwd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Processing documents into tokens"
      ],
      "metadata": {
        "id": "2p8qRNvnLCTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "porter = PorterStemmer()\n",
        "\n",
        "def tokenizer(text):\n",
        "    return text.split()\n",
        "\n",
        "\n",
        "def tokenizer_porter(text):\n",
        "    return [porter.stem(word) for word in text.split()]\n",
        "\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "[w for w in tokenizer_porter('a runner likes running and runs a lot')\n",
        " if w not in stop]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnf0K2OJK10j",
        "outputId": "385d2a10-e539-4d78-97d4-db1066573570"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['runner', 'like', 'run', 'run', 'lot']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training a logistic regression model for document classification"
      ],
      "metadata": {
        "id": "9191_PeLLesk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = df.loc[:25000, 'review'].values\n",
        "y_train = df.loc[:25000, 'sentiment'].values\n",
        "X_test = df.loc[25000:, 'review'].values\n",
        "y_test = df.loc[25000:, 'sentiment'].values\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "tfidf = TfidfVectorizer(strip_accents=None,\n",
        "                        lowercase=False,\n",
        "                        preprocessor=None)\n",
        "\n",
        "\"\"\"\n",
        "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
        "               'vect__stop_words': [stop, None],\n",
        "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
        "               'clf__penalty': ['l1', 'l2'],\n",
        "               'clf__C': [1.0, 10.0, 100.0]},\n",
        "              {'vect__ngram_range': [(1, 1)],\n",
        "               'vect__stop_words': [stop, None],\n",
        "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
        "               'vect__use_idf':[False],\n",
        "               'vect__norm':[None],\n",
        "               'clf__penalty': ['l1', 'l2'],\n",
        "               'clf__C': [1.0, 10.0, 100.0]},\n",
        "              ]\n",
        "\"\"\"\n",
        "\n",
        "small_param_grid = [{'vect__ngram_range': [(1, 1)],\n",
        "                     'vect__stop_words': [None],\n",
        "                     'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
        "                     'clf__penalty': ['l2'],\n",
        "                     'clf__C': [1.0, 10.0]},\n",
        "                    {'vect__ngram_range': [(1, 1)],\n",
        "                     'vect__stop_words': [stop, None],\n",
        "                     'vect__tokenizer': [tokenizer],\n",
        "                     'vect__use_idf':[False],\n",
        "                     'vect__norm':[None],\n",
        "                     'clf__penalty': ['l2'],\n",
        "                  'clf__C': [1.0, 10.0]},\n",
        "              ]\n",
        "\n",
        "lr_tfidf = Pipeline([('vect', tfidf),\n",
        "                     ('clf', LogisticRegression(solver='liblinear'))])\n",
        "\n",
        "gs_lr_tfidf = GridSearchCV(lr_tfidf, small_param_grid,\n",
        "                           scoring='accuracy',\n",
        "                           cv=5,\n",
        "                           verbose=1,\n",
        "                           n_jobs=-1)"
      ],
      "metadata": {
        "id": "CHloKgqVLWtM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p>param_grid consisted of two parameter dictionaries. In the first dictionary, we used TfidfVectorizer with its default settings (use_idf=True, smooth_idf=True, and norm='l2') to calculate the tf-idfs; in the second dictionary, we set those parameters to use_idf=False, smooth_idf=False, and norm=None in order to train a model based on raw term frequencies.</p>"
      ],
      "metadata": {
        "id": "7vZvH69iL_sX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gs_lr_tfidf.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5AV_mgqLuKD",
        "outputId": "790783e1-cc69-4561-f79d-57857ea4e3cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Best parameter set: {gs_lr_tfidf.best_params_}')\n",
        "print(f'CV Accuracy: {gs_lr_tfidf.best_score_:.3f}')"
      ],
      "metadata": {
        "id": "yDqvkV_JMG39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = gs_lr_tfidf.best_estimator_\n",
        "print(f'Test Accuracy: {clf.score(X_test, y_test):.3f}')"
      ],
      "metadata": {
        "id": "7P5BQMpeM_kH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Start comment:\n",
        "Please note that gs_lr_tfidf.best_score_ is the average k-fold cross-validation score. I.e., if we have a GridSearchCV object with 5-fold cross-validation (like the one above), the best_score_ attribute returns the average score over the 5-folds of the best model. To illustrate this with an example:"
      ],
      "metadata": {
        "id": "Y9RABcFyNWZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "np.random.seed(0)\n",
        "np.set_printoptions(precision=6)\n",
        "y = [np.random.randint(3) for i in range(25)]\n",
        "X = (y + np.random.randn(25)).reshape(-1, 1)\n",
        "\n",
        "cv5_idx = list(StratifiedKFold(n_splits=5, shuffle=False).split(X, y))\n",
        "    \n",
        "lr = LogisticRegression()\n",
        "cross_val_score(lr, X, y, cv=cv5_idx)"
      ],
      "metadata": {
        "id": "ohB0eG9yNXlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "lr = LogisticRegression()\n",
        "gs = GridSearchCV(lr, {}, cv=cv5_idx, verbose=3).fit(X, y) "
      ],
      "metadata": {
        "id": "EE7kUVOLNlKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Working with bigger data - online algorithms and out-of-core learning"
      ],
      "metadata": {
        "id": "MOo0tWfHNvK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell is not contained in the book but\n",
        "# added for convenience so that the notebook\n",
        "# can be executed starting here, without\n",
        "# executing prior code in this notebook\n",
        "\n",
        "import os\n",
        "import gzip\n",
        "\n",
        "\n",
        "if not os.path.isfile('movie_data.csv'):\n",
        "    if not os.path.isfile('movie_data.csv.gz'):\n",
        "        print('Please place a copy of the movie_data.csv.gz'\n",
        "              'in this directory. You can obtain it by'\n",
        "              'a) executing the code in the beginning of this'\n",
        "              'notebook or b) by downloading it from GitHub:'\n",
        "              'https://github.com/rasbt/machine-learning-book/'\n",
        "              'blob/main/ch08/movie_data.csv.gz')\n",
        "    else:\n",
        "        with gzip.open('movie_data.csv.gz', 'rb') as in_f, \\\n",
        "                open('movie_data.csv', 'wb') as out_f:\n",
        "            out_f.write(in_f.read())"
      ],
      "metadata": {
        "id": "ooIjfbP5NwOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since not everyone has access to supercomputer facilities, we will now apply a technique called out-of-core learning, which allows us to work with such large datasets by fitting the classifier incrementally on smaller batches of a datase"
      ],
      "metadata": {
        "id": "M6PNw-hxOa8V"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SlZZvu5WObmE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}